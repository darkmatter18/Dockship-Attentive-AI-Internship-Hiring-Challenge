{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Training](#Training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CROP_SIZE = 256\n",
    "IMAGE_SIZE = 224\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "LR = 0.0001\n",
    "EPOCHS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.jpg</td>\n",
       "      <td>Adhered</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.jpg</td>\n",
       "      <td>Adhered</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.jpg</td>\n",
       "      <td>Concrete</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.jpg</td>\n",
       "      <td>Concrete</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.jpg</td>\n",
       "      <td>Plastic &amp; fabric</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0                 1\n",
       "0  0.jpg           Adhered\n",
       "1  1.jpg           Adhered\n",
       "2  2.jpg          Concrete\n",
       "3  3.jpg          Concrete\n",
       "4  4.jpg  Plastic & fabric"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading the Datafarme\n",
    "data = pd.read_csv('./dataset/train_challenge.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Classes\n",
    "CLASSES = ['Adhered', 'Ballasted', 'Concrete', 'Plastic & fabric', 'Shingle', 'Steel']\n",
    "\n",
    "classes_to_idx = {cls: idx for idx, cls in enumerate(CLASSES)}\n",
    "idx_to_classes = {idx: cls for idx, cls in enumerate(CLASSES)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.jpg</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.jpg</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.jpg</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0  1\n",
       "0  0.jpg  0\n",
       "1  1.jpg  0\n",
       "2  2.jpg  2\n",
       "3  3.jpg  2\n",
       "4  4.jpg  3"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# replace '1' column with classes_to_idx dict\n",
    "data = data.replace({'1': classes_to_idx})\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make X and Y from dataframe\n",
    "X = data['0'].values\n",
    "Y = data['1'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 6915\n",
      "Validation size: 1729\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Split into train, validation and test sets\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, Y, test_size=0.2)\n",
    "\n",
    "print(f\"Train size: {len(X_train)}\\nValidation size: {len(X_val)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Torch Dataset and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SatelliteDataset(Dataset):\n",
    "    def __init__(self, dataroot: str, X_array: np.array, Y_array: np.array, transform = None, target_transform = None):\n",
    "        self.dataroot = dataroot\n",
    "        self.X_array = X_array\n",
    "        self.Y_array = Y_array\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        file_name = self.X_array[index]\n",
    "        img = Image.open(os.path.join(self.dataroot, file_name)).convert('RGB')\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "            \n",
    "        label = np.array(self.Y_array[index])\n",
    "        if self.target_transform is not None:\n",
    "            label = self.target_transform(label)\n",
    "        else:\n",
    "            label = torch.from_numpy(label)\n",
    "\n",
    "        return {'image': img, 'label': label, 'image_name': file_name}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SatelliteDataset(dataroot='./dataset/train/', X_array=X_train, Y_array=y_train, \n",
    "                                 transform=transforms.Compose([\n",
    "                                                               transforms.Resize(CROP_SIZE),\n",
    "                                                               transforms.RandomHorizontalFlip(),\n",
    "                                                               transforms.CenterCrop(IMAGE_SIZE),\n",
    "                                                               transforms.ToTensor(),\n",
    "                                                               transforms.Normalize([0.4728, 0.4762, 0.4692],\n",
    "                                                                                    [0.2558, 0.2532, 0.2457])]))\n",
    "\n",
    "val_dataset = SatelliteDataset(dataroot='./dataset/train/', X_array=X_train, Y_array=y_train, \n",
    "                               transform=transforms.Compose([transforms.Resize(CROP_SIZE),\n",
    "                                                             transforms.CenterCrop(IMAGE_SIZE),\n",
    "                                                             transforms.ToTensor(),\n",
    "                                                             transforms.Normalize([0.4728, 0.4762, 0.4692],\n",
    "                                                                                  [0.2558, 0.2532, 0.2457])]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "valloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Tesla T4 for Training\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Using {torch.cuda.get_device_name()} for Training\")\n",
    "else:\n",
    "    print(\"Using CPU for Training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResnetModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResnetModel, self).__init__()\n",
    "        r = resnet18(pretrained=True)\n",
    "        fc = nn.Linear(r.fc.in_features, len(CLASSES))\n",
    "        r.fc = fc\n",
    "        self.model = r\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResnetModel(\n",
       "  (model): ResNet(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (fc): Linear(in_features=512, out_features=6, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ResnetModel()\n",
    "model = model.to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss\n",
    "criterion = nn.NLLLoss().cuda() if torch.cuda.is_available() else nn.NLLLoss()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LR scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 1.0000e-04.\n"
     ]
    }
   ],
   "source": [
    "scheduler = lr_scheduler.StepLR(optimizer, 5, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model for Epoch 1\n",
      "Saving improved Model for Epoch 1\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[Epoch: 1/20] training loss: 0.8877061809479803 validation loss: 0.5011057774161224, accuracy: 0.8229934924078091\n",
      "Saving model for Epoch 2\n",
      "Saving improved Model for Epoch 2\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[Epoch: 2/20] training loss: 0.43920582476855186 validation loss: 0.33033157908940175, accuracy: 0.8896601590744758\n",
      "Saving model for Epoch 3\n",
      "Saving improved Model for Epoch 3\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[Epoch: 3/20] training loss: 0.33202257379620126 validation loss: 0.27630143846753535, accuracy: 0.9125090383224873\n",
      "Saving model for Epoch 4\n",
      "Saving improved Model for Epoch 4\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[Epoch: 4/20] training loss: 0.24241272422255422 validation loss: 0.22571264357667684, accuracy: 0.9181489515545914\n",
      "Saving model for Epoch 5\n",
      "Saving improved Model for Epoch 5\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "[Epoch: 5/20] training loss: 0.23744248069892823 validation loss: 0.16696842916239799, accuracy: 0.9498192335502531\n",
      "Saving model for Epoch 6\n",
      "Saving improved Model for Epoch 6\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "[Epoch: 6/20] training loss: 0.16233537022027086 validation loss: 0.14556120929731808, accuracy: 0.9589298626174982\n",
      "Saving model for Epoch 7\n",
      "Saving improved Model for Epoch 7\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "[Epoch: 7/20] training loss: 0.15005612316462585 validation loss: 0.13655733600614386, accuracy: 0.9645697758496024\n",
      "Saving model for Epoch 8\n",
      "Model is not improved for this time\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "[Epoch: 8/20] training loss: 0.14381242473316813 validation loss: 0.13441127499892758, accuracy: 0.9641359363702097\n",
      "Saving model for Epoch 9\n",
      "Saving improved Model for Epoch 9\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "[Epoch: 9/20] training loss: 0.1420778175065764 validation loss: 0.12899547542027176, accuracy: 0.9660159074475777\n",
      "Saving model for Epoch 10\n",
      "Saving improved Model for Epoch 10\n",
      "Adjusting learning rate of group 0 to 1.0000e-06.\n",
      "[Epoch: 10/20] training loss: 0.13712990175789883 validation loss: 0.12139535169625576, accuracy: 0.9680404916847433\n",
      "Saving model for Epoch 11\n",
      "Model is not improved for this time\n",
      "Adjusting learning rate of group 0 to 1.0000e-06.\n",
      "[Epoch: 11/20] training loss: 0.12958941618256697 validation loss: 0.1323022831912844, accuracy: 0.9634128705712219\n",
      "Saving model for Epoch 12\n",
      "Model is not improved for this time\n",
      "Adjusting learning rate of group 0 to 1.0000e-06.\n",
      "[Epoch: 12/20] training loss: 0.13210092940713217 validation loss: 0.12454580575960404, accuracy: 0.9658712942877802\n",
      "Saving model for Epoch 13\n",
      "Saving improved Model for Epoch 13\n",
      "Adjusting learning rate of group 0 to 1.0000e-06.\n",
      "[Epoch: 13/20] training loss: 0.1283978653057537 validation loss: 0.12117780094679421, accuracy: 0.9689081706435285\n",
      "Saving model for Epoch 14\n",
      "Model is not improved for this time\n",
      "Adjusting learning rate of group 0 to 1.0000e-06.\n",
      "[Epoch: 14/20] training loss: 0.1303809016921429 validation loss: 0.1208905328035527, accuracy: 0.9680404916847433\n",
      "Saving model for Epoch 15\n",
      "Saving improved Model for Epoch 15\n",
      "Adjusting learning rate of group 0 to 1.0000e-07.\n",
      "[Epoch: 15/20] training loss: 0.1293064616335016 validation loss: 0.11827087154422045, accuracy: 0.9696312364425163\n",
      "Saving model for Epoch 16\n",
      "Model is not improved for this time\n",
      "Adjusting learning rate of group 0 to 1.0000e-07.\n",
      "[Epoch: 16/20] training loss: 0.12737029758985716 validation loss: 0.12231863729240923, accuracy: 0.968474331164136\n",
      "Saving model for Epoch 17\n",
      "Model is not improved for this time\n",
      "Adjusting learning rate of group 0 to 1.0000e-07.\n",
      "[Epoch: 17/20] training loss: 0.12439393024899696 validation loss: 0.12735942516176924, accuracy: 0.967172812725958\n",
      "Saving model for Epoch 18\n",
      "Model is not improved for this time\n",
      "Adjusting learning rate of group 0 to 1.0000e-07.\n",
      "[Epoch: 18/20] training loss: 0.1291021788025454 validation loss: 0.12213129551296034, accuracy: 0.968474331164136\n",
      "Saving model for Epoch 19\n",
      "Model is not improved for this time\n",
      "Adjusting learning rate of group 0 to 1.0000e-07.\n",
      "[Epoch: 19/20] training loss: 0.1265910539606388 validation loss: 0.12966346809140858, accuracy: 0.9645697758496024\n"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "test_losses = []\n",
    "accuracies = []\n",
    "better_accuracy = 0.0\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    #Training\n",
    "    train_loss = 0.0\n",
    "    test_loss = 0.0\n",
    "    accuracy = 0.0\n",
    "    \n",
    "    model.train()\n",
    "    for data in trainloader:\n",
    "        x = data['image'].to(device)\n",
    "        y = data['label'].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(x)\n",
    "        loss = criterion(out, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * x.size(0)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data in valloader:\n",
    "            x = data['image'].to(device)\n",
    "            y = data['label'].to(device)\n",
    "            out = model(x)\n",
    "            loss = criterion(out, y)\n",
    "            test_loss += loss.item() * x.size(0)\n",
    "            accuracy += accuracy_score(y.cpu().numpy(), torch.exp(out).argmax(1).cpu().numpy()) * x.size(0)\n",
    "        \n",
    "    train_loss /= len(trainloader.dataset)\n",
    "    test_loss /= len(valloader.dataset)\n",
    "    accuracy /= len(valloader.dataset)\n",
    "    train_losses.append(train_loss)\n",
    "    test_losses.append(test_loss)\n",
    "    accuracies.append(accuracy)\n",
    "    \n",
    "    print(f'Saving model for Epoch {epoch}')\n",
    "    torch.save(model.state_dict(), f'./models/model_{epoch}.pt')\n",
    "    \n",
    "    if accuracy > better_accuracy:\n",
    "        better_accuracy = accuracy\n",
    "        torch.save(model.state_dict(), './models/model_best.pt')\n",
    "        print(f\"Saving improved Model for Epoch {epoch}\")\n",
    "    else:\n",
    "        print(\"Model is not improved for this time\")\n",
    "    \n",
    "    if scheduler is None:\n",
    "        print(\"No Schedular found. LR will not change\")\n",
    "    else:\n",
    "        if isinstance(scheduler, lr_scheduler.ReduceLROnPlateau):\n",
    "            scheduler.step(test_loss)\n",
    "        else:\n",
    "            scheduler.step()\n",
    "        \n",
    "    print(f\"[Epoch: {epoch}/{EPOCHS}] training loss: {train_loss} validation loss: {test_loss}, accuracy: {accuracy}\")\n",
    "    \n",
    "print(\"End of training!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_losses)\n",
    "plt.plot(test_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f2a25c3edd0>]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0138728725030985,\n",
       " 0.4576777295107783,\n",
       " 0.35864640212283133,\n",
       " 0.2987881768723803,\n",
       " 0.2255530750760115,\n",
       " 0.212126887473867,\n",
       " 0.19999943744291537,\n",
       " 0.19310804978497892,\n",
       " 0.1862424666822948,\n",
       " 0.18614838275026774,\n",
       " 0.18638731699789768,\n",
       " 0.18402824462192782,\n",
       " 0.1839700485689435,\n",
       " 0.18447449869292418,\n",
       " 0.18505550535881943,\n",
       " 0.18498952816296382,\n",
       " 0.18436532563604657,\n",
       " 0.1850129894044894,\n",
       " 0.18461198096506334,\n",
       " 0.18250264148305312]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5601876224587125,\n",
       " 0.3477129156617091,\n",
       " 0.27352861835960024,\n",
       " 0.250325299345835,\n",
       " 0.20851717106216813,\n",
       " 0.19224109280846383,\n",
       " 0.19147077230541326,\n",
       " 0.18022449408117103,\n",
       " 0.17901863205210936,\n",
       " 0.17890834109460368,\n",
       " 0.17647851671288173,\n",
       " 0.184427017871013,\n",
       " 0.18164609727151396,\n",
       " 0.17806966908798647,\n",
       " 0.1752669436931179,\n",
       " 0.17696222002447642,\n",
       " 0.17952873134421252,\n",
       " 0.1809442790242657,\n",
       " 0.17564729467163634,\n",
       " 0.18155874904791944]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8008676789587853,\n",
       " 0.8880694143167028,\n",
       " 0.9107736804049168,\n",
       " 0.9250903832248735,\n",
       " 0.9347794649313087,\n",
       " 0.93940708604483,\n",
       " 0.9415762834417932,\n",
       " 0.9472161966738973,\n",
       " 0.9485177151120752,\n",
       " 0.9466377440347071,\n",
       " 0.9476500361532899,\n",
       " 0.9443239334779465,\n",
       " 0.9420101229211858,\n",
       " 0.9479392624728851,\n",
       " 0.9498192335502531,\n",
       " 0.9490961677512654,\n",
       " 0.9466377440347071,\n",
       " 0.9443239334779465,\n",
       " 0.9505422993492407,\n",
       " 0.9457700650759219]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing on training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = np.empty(0)\n",
    "y_pred = np.empty(0)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        x = data['image'].to(device)\n",
    "        y = data['label'].to(device)\n",
    "        out = model(x)\n",
    "        out = torch.exp(out).argmax(1)\n",
    "        y_true = np.append(y_true, y.cpu().numpy())\n",
    "        y_pred = np.append(y_pred, out.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_true, y_pred, target_names=CLASSES))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0  1\n",
       "0  0.jpg  0\n",
       "1  1.jpg  0\n",
       "2  2.jpg  0\n",
       "3  3.jpg  0\n",
       "4  4.jpg  0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading the Datafarme\n",
    "test_data = pd.read_csv('./dataset/test_challenge.csv')\n",
    "test_data = test_data.replace({'1': classes_to_idx})\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "CENTER_CROP_FIVE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CENTER_CROP_FIVE:\n",
    "    testset = SatelliteDataset(dataroot='./dataset/test/', X_array=test_data['0'].values, Y_array=test_data['1'].values, \n",
    "                               transform=transforms.Compose([transforms.Resize(256),\n",
    "                                                             transforms.FiveCrop(IMAGE_SIZE),\n",
    "                                                             transforms.Lambda(lambda crops: torch.stack([\n",
    "                                                                 transforms.Normalize([0.4728, 0.4762, 0.4692],\n",
    "                                                                                      [0.2558, 0.2532, 0.2457])(\n",
    "                                                                     transforms.ToTensor()(crop)) for crop in crops]))\n",
    "                                                            ]))\n",
    "\n",
    "else:\n",
    "    testset = SatelliteDataset(dataroot='./dataset/test/', X_array=test_data['0'].values, Y_array=test_data['1'].values, \n",
    "                               transform=transforms.Compose([transforms.Resize(CROP_SIZE),\n",
    "                                                             transforms.CenterCrop(IMAGE_SIZE),\n",
    "                                                             transforms.ToTensor(),\n",
    "                                                             transforms.Normalize([0.4728, 0.4762, 0.4692],\n",
    "                                                                                  [0.2558, 0.2532, 0.2457])]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "testsetloader = DataLoader(testset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "596"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(testsetloader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infarence_and_save(epoch):\n",
    "    model.load_state_dict(torch.load(f'./models/model_{epoch}.pt'))\n",
    "\n",
    "    y_test_pred = np.empty(0)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data in testsetloader:\n",
    "            x = data['image'].to(device)\n",
    "            if CENTER_CROP_FIVE:\n",
    "                bs, ncrops, c, h, w = x.size()\n",
    "                out = torch.exp(model(x.view(-1, c, h, w)))\n",
    "                out, _ = out.argmax(1).view(bs, ncrops).median(1)\n",
    "                y_test_pred = np.append(y_test_pred, out.cpu().numpy())\n",
    "            else:\n",
    "                out = model(x)\n",
    "                out = torch.exp(out).argmax(1)\n",
    "                y_test_pred = np.append(y_test_pred, out.cpu().numpy())\n",
    "\n",
    "    y_test_pred.shape\n",
    "\n",
    "    d = {'0': test_data['0'].values, '1': y_test_pred.astype(int)}\n",
    "    pd.DataFrame(d).replace({'1': idx_to_classes}).to_csv(f'./output_{epoch}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working for epoch 1\n",
      "Working for epoch 2\n",
      "Working for epoch 3\n",
      "Working for epoch 4\n",
      "Working for epoch 5\n",
      "Working for epoch 6\n",
      "Working for epoch 7\n",
      "Working for epoch 8\n",
      "Working for epoch 9\n",
      "Working for epoch 10\n",
      "Working for epoch 11\n",
      "Working for epoch 12\n",
      "Working for epoch 13\n",
      "Working for epoch 14\n",
      "Working for epoch 15\n",
      "Working for epoch 16\n",
      "Working for epoch 17\n",
      "Working for epoch 18\n",
      "Working for epoch 19\n",
      "Working for epoch 20\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 21):\n",
    "    print(f\"Working for epoch {i}\")\n",
    "    infarence_and_save(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-7.mnightly-2021-01-20-debian-10-test",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-7:mnightly-2021-01-20-debian-10-test"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
